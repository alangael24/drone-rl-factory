# Security Architecture: Doctrina de Desconfianza M√°xima

## El Problema

La IA puede alucinarse a s√≠ misma:
```
LLM genera f√≠sica ROTA + LLM escribe test ROTO que la aprueba
‚Üì
Sistema dice "Todo OK" ‚úÖ
‚Üì
Entrenam...os 10 horas
‚Üì
Resultado: Basura üóëÔ∏è
Costo: $500-2000 en GPU
```

## La Soluci√≥n: 3 Candados Financieros

El sistema **NUNCA** inicia entrenamiento masivo sin validaci√≥n en 4 capas. Cada capa aborta si falla.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ARQUITECTURA SEGURA                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. COMPILACI√ìN
   ‚îî‚îÄ‚Üí ¬øC√≥digo C v√°lido?
       ‚îú‚îÄ S√ç ‚Üí Continuar
       ‚îî‚îÄ NO ‚Üí ABORT (costo: 1s)

2. CANDADO 1: TESTS F√çSICOS
   ‚îî‚îÄ‚Üí ¬øPasa 8 tests de f√≠sica?
       ‚îú‚îÄ Tests: compilaci√≥n, estabilidad, gravedad,
       ‚îÇ          energ√≠a, l√≠mites, determinismo, validez sem√°ntica
       ‚îú‚îÄ S√ç ‚Üí Continuar
       ‚îî‚îÄ NO ‚Üí ABORT (costo: 10s) + LLM auto-corrige

3. CANDADO 2: VISTA PREVIA (5 segundos)
   ‚îî‚îÄ‚Üí ¬øSe ve coherente?
       ‚îú‚îÄ Simula 5s con acciones aleatorias
       ‚îú‚îÄ Mostrarprogreso cada 10 pasos
       ‚îú‚îÄ Chequeos por dominio:
       ‚îÇ   - Drone: z >= 0 (no bajo tierra)
       ‚îÇ   - CartPole: √°ngulo en [-œÄ, œÄ]
       ‚îÇ   - WarehouseRobot: dentro de l√≠mites
       ‚îÇ   - RoboticArm: rangos articulares v√°lidos
       ‚îú‚îÄ S√ç ‚Üí Continuar
       ‚îî‚îÄ NO ‚Üí ABORT (costo: 5s)

4. CANDADO 3: DRY RUN (1000 pasos ‚âà 10 segundos)
   ‚îî‚îÄ‚Üí ¬øHay se√±al de aprendizaje?
       ‚îú‚îÄ Calcula: learning_gain = (R_final - R_init) / |R_init|
       ‚îú‚îÄ Si learning_gain >= 1%: continuar
       ‚îú‚îÄ Si learning_gain < 1% Y recompensa plana: ABORT
       ‚îî‚îÄ ABORT (costo: 10s vs 10 horas de GPU)

5. ENTRENAMIENTO MASIVO
   ‚îî‚îÄ‚Üí Si pas√≥ los 4 candados anterior
       ‚îî‚îÄ Entrena hasta completion
```

## Cost Analysis

| Escenario | Costo | Protecci√≥n |
|-----------|-------|-----------|
| F√≠sica rota | $0.01 | ‚úÖ Detectada en test 1 |
| Test roto | $0.10 | ‚úÖ Detectado en dry run |
| Teletransporte | $0.10 | ‚úÖ Vista previa 5s |
| Recompensa imposible | $0.50 | ‚úÖ Early stopping 1000 pasos |
| **Entrenamiento roto** | **$1000+** | ‚úÖ NUNCA SUCEDE |

## Implementaci√≥n

### 1. Candado Financiero: Abort on Fail

**Archivo:** `universal_platform.py`

```python
if not report.all_tests_passed:
    print(f"[ERROR CR√çTICO] C√≥digo no pas√≥ verificaci√≥n")
    for hint in report.correction_hints:
        print(f"  ‚Üí {hint}")
    raise RuntimeError(f"Verificaci√≥n fallida. No se iniciar√° el entrenamiento.")
```

**Efecto:** Si la f√≠sica falla ANY test, el programa se detiene inmediatamente.

---

### 2. Candalo Visual: Preview Physics

**Archivo:** `preview_physics.py`

```python
def preview_physics(domain, physics_code, reward_code, verify_code, duration=5.0):
    """
    Ejecuta 5 segundos de simulaci√≥n y analiza:
    - ¬øHay NaN/infinitos?
    - ¬øEl robot se mueve?
    - ¬øLas recompensas son razonables?
    - Chequeos espec√≠ficos por dominio
    """
    ...
    return approved, diagnosis
```

**Ejecuci√≥n:**
```
Paso   0 | Obs: [0.00, 0.02, -0.02] | Reward: +0.942
Paso  10 | Obs: [0.02, 0.05, -0.43] | Reward: +0.508
Paso  20 | Obs: [-0.00, -0.06, 0.08] | Reward: +0.825

An√°lisis:
‚úÖ Movimiento total: 0.88 unidades
‚úÖ Recompensa media: 0.616 (razonable)
‚úÖ No hay NaN/infinitos
‚úÖ LA F√çSICA SE VE COHERENTE
```

---

### 3. Candado de Convergencia: Early Stopping

**Archivo:** `simple_ppo.py`

```python
def train_with_early_stopping(self, total_steps, dry_run_steps=1000):
    """
    Fase 1: Dry run de 1000 pasos
    - learning_gain = (reward_final - reward_init) / |reward_init|

    Si learning_gain < 1% y recompensa plana ‚Üí ABORT
    Si hay progreso ‚Üí contin√∫a hasta total_steps
    """

    # Dry run
    metrics_dry = self.train(dry_run_steps)
    learning_gain = (metrics_dry.mean_reward_final -
                     metrics_dry.mean_reward_initial) / ...

    if learning_gain < 0.01:
        return metrics, False, "Recompensa imposible de aprender"

    # Fase 2: Entrenamiento masivo (si pas√≥ dry run)
    metrics_final = self.train(total_steps - dry_run_steps)
    return metrics_final, True, "OK"
```

---

## Flujo de Seguridad Completo

```mermaid
graph TD
    A["LLM genera c√≥digo"] --> B["Compilaci√≥n"]
    B -->|Falla| ABORT1["ABORT (costo: $0.01)"]
    B -->|OK| C["Tests de F√≠sica"]
    C -->|Falla| ABORT2["ABORT (costo: $0.10)"]
    C -->|OK| D["Vista Previa 5s"]
    D -->|Falla| ABORT3["ABORT (costo: $0.10)"]
    D -->|OK| E["Dry Run 1000 pasos"]
    E -->|learning_gain < 1%| ABORT4["ABORT (costo: $0.50)"]
    E -->|OK| F["Entrenamiento Masivo"]
    F --> G["Modelo Guardado"]

    ABORT1 --> H["LLM recibe diagn√≥stico"]
    ABORT2 --> H
    ABORT3 --> H
    ABORT4 --> H
    H --> I["Auto-corrige c√≥digo"]
    I --> A
```

---

## Garant√≠as

‚úÖ **NUNCA** entrenar√°s algo roto (4 capas de validaci√≥n)
‚úÖ **NUNCA** gastar√°s >$10 en algo imposible (early stopping)
‚úÖ **NUNCA** perder√°s horas esperando a que falle
‚úÖ **SIEMPRE** ver√°s diagn√≥sticos espec√≠ficos para arreglar

---

## Ejemplo: Detecci√≥n Temprana

Si el LLM genera un drone que vuela al rev√©s:

```
Phase 1: Compilaci√≥n ‚úÖ
Phase 2: Tests de F√≠sica ‚úÖ
Phase 3: Vista Previa
  Paso 0: Obs: [0.0, 0.0, 0.5, ...] | Reward: +0.5
  Paso 10: Obs: [-50, -50, -100, ...]  ‚Üê ANOMAL√çA

‚ùå ABORT: "Robot sali√≥ de l√≠mites razonables"

LLM recibe:
  "El drone vuela muy bajo. A√±adir: if (z < 0) return 0;"

LLM auto-corrige ‚Üí Reintentar
```

---

## Costo Total M√°ximo (Peor Caso)

| Capa | Costo | Tiempo |
|------|-------|--------|
| Compilaci√≥n | $0.01 | 1s |
| Tests f√≠sicos | $0.02 | 5s |
| Vista previa | $0.05 | 5s |
| Dry run | $0.20 | 10s |
| **TOTAL** | **$0.28** | **21s** |

vs. Entrenamiento masivo fallido: **$1000+** / **10 horas**

---

## Integraci√≥n en Pipeline

```python
# universal_platform.py

def verified_train(...):
    # Fase 1: Generaci√≥n
    physics = architect.generate_physics(domain)
    reward = architect.generate_reward(domain, task)
    verify = architect.generate_verification(domain, physics)

    # Fase 2: Compilaci√≥n + Tests ‚Üí ABORT si falla
    report = verifier.verify(domain, physics, reward, verify)
    if not report.all_tests_passed:
        raise RuntimeError("Verificaci√≥n fallida")  # ‚Üê CANDADO 1

    # Fase 2.5: Vista previa ‚Üí ABORT si falla
    approved, _ = preview_physics(domain, physics, reward, verify)
    if not approved:
        raise RuntimeError("Vista previa rechazada")  # ‚Üê CANDADO 2

    # Fase 3: Entrenamiento con early stopping ‚Üí ABORT si falla
    metrics, approved, _ = ppo.train_with_early_stopping(steps)
    if not approved:
        raise RuntimeError("Early stopping")  # ‚Üê CANDADO 3

    # Fase 4: Guardado (SOLO si pas√≥ todo)
    torch.save(model, path)
```

---

## Resumen para Equipos de RL

> "Cada peso cuesta dinero. Cada hora cuesta dinero.
>
> Antes implement√°bamos: Generar ‚Üí Entrenar ‚Üí Oops
>
> Ahora implementamos: Generar ‚Üí Validar ‚Üí Validar ‚Üí Validar ‚Üí Entrenar
>
> Coste de validaci√≥n: $0.28
> Coste de entrenamiento roto: $1000
>
> El costo/beneficio es 1:3500"

---

**Principio fundamental:**
*"No conf√≠es en la IA. Verifica 4 veces. Entrena 1 vez."*
